{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2018bd70",
   "metadata": {},
   "source": [
    "# Data Connection\n",
    "\n",
    "## 1. Data Loader - Load a PDF document (2 ways)\n",
    "## 2. Document transformers - multiple options\n",
    "## 3. Embeddings from Open AI\n",
    "## 4. Vector Store - Chroma DB, store \n",
    "## 5. Vector Store - Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446fc2f",
   "metadata": {},
   "source": [
    "##  1A. Ways to load a PDF document\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
    "\n",
    "\n",
    "### 1Aa - Using PyPDF\n",
    "\n",
    "PyPDF is a popular library used to work with PDF files in Python. It can be used to extract text, metadata, and other information from PDF documents. Here's how Langchain utilizes PyPDF:\n",
    "\n",
    "- **Loading PDFs**: PyPDF is used to load PDF documents into an array of documents, where each document contains the page content and metadata with the page number.\n",
    "- **Installation**: PyPDF can be installed using `pip install pypdf`.\n",
    "- **Usage**: PyPDFLoader is used to load and split the PDF into pages, and the content can be accessed as shown in the code snippet on the webpage.\n",
    "- **Advantage**: An advantage of using PyPDF is that documents can be retrieved with page numbers, allowing for easy navigation and reference.\n",
    "\n",
    "### 1Ab - Using Unstructured\n",
    "\n",
    "Unstructured is another method mentioned on the webpage for loading PDF documents. Here's how it works:\n",
    "\n",
    "- **Loading PDFs**: UnstructuredPDFLoader is used to load PDF documents.\n",
    "- **Elements Handling**: Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default, these are combined together, but separation can be maintained by specifying `mode=\"elements\"`.\n",
    "- **Usage**: UnstructuredPDFLoader is used to load the PDF, and the content can be accessed similarly to PyPDF.\n",
    "\n",
    "### When to Use PyPDF vs. Unstructured\n",
    "\n",
    "- **PyPDF**:\n",
    "  - **When Page Numbers are Important**: If you need to keep track of page numbers and want to work with individual pages, PyPDF is a suitable choice.\n",
    "  - **General Text Extraction**: PyPDF is widely used for general text extraction from PDF documents and has a well-established community.\n",
    "\n",
    "- **Unstructured**:\n",
    "  - **Handling Different Text Elements**: If you need to work with different chunks of text and want to retain the separation between different elements, Unstructured might be a better option.\n",
    "  - **Customized Text Processing**: Unstructured allows for more customized handling of text elements, making it suitable for specific use cases where text needs to be processed in a particular way.\n",
    "\n",
    "### Selection\n",
    "\n",
    "The choice between PyPDF and Unstructured depends on the specific requirements of your task. If you need general text extraction with page numbers, PyPDF is a solid choice. If you require more customized handling of text elements, Unstructured might be more suitable. Both methods are supported by Langchain, allowing for flexibility in handling PDF documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd6300",
   "metadata": {},
   "source": [
    "### 1Aa. PyPDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73378eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1360822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fb3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('data/Causal_Inference_in_Python.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1804cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38488b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 399)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages), len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb74222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    page.page_content = page.page_content.encode('utf-8', 'replace').decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2044f0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Front Door Adjustment\n",
      "The backdoor adjustment is not the only possible strategy to identify causal effects.\n",
      "One can leverage the knowledge of causal mechanisms to identify the causal effect via\n",
      "a front door, even in the presence of unmeasured common causes:\n",
      "With this strategy, you must be able to identify the effect of the treatment on a media‐\n",
      "tor and the effect of that mediator on the outcome. Then, the identification of the\n",
      "effect of treatment on the outcome becomes the combination of those two effects.\n",
      "However, in the tech industry, it’s hard to find applications where such a graph is\n",
      "plausible, which is why the front door adjustment is not so popular.\n",
      "Confounding Bias\n",
      "The first significant cause of bias  is confounding. It’s the bias we’ve been discussing so\n",
      "far. Now, we are just putting a name to it. Confounding happens when there is an open\n",
      "backdoor path through which association flows  noncausally, usually because the treat‐\n",
      "ment and the outcome share a common cause . For example, let’s say that you work in\n",
      "HR and you want to know if your new management training program is increasing\n",
      "employers’ engagement. However, since the training is optional, you believe only\n",
      "managers that are already doing great attend the program and those who need it the\n",
      "most, don’t. When you measure engagement of the teams under the managers that\n",
      "took the training, it is much higher than that of the teams under the managers who\n",
      "didn’t attend the training. But it’s hard to know how much of this is causal. Since\n",
      "there is a common cause between treatment and outcome, they would move together\n",
      "regardless of a causal effect.\n",
      "To identify that causal effect, you need to close all backdoor paths between the treat‐\n",
      "ment and the outcome. If you do so, the only effect that will be left is the direct effectT Y. In our example, you could somehow control for the manager’s quality prior\n",
      "to taking the training. In that situation, the difference in the outcome will be only due\n",
      "to the training, since manager quality prior to the training would be held constant\n",
      "between treatment and control. Simply put, to adjust for confounding bias, you need to\n",
      "adjust for the common causes of the treatment and the outcome :80 | Chapter 3: Graphical Causal Models\n"
     ]
    }
   ],
   "source": [
    "print (pages[100].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cf4c3",
   "metadata": {},
   "source": [
    "### 1Ab. Unstructured approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7baef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24680e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2 = UnstructuredPDFLoader(\"data/Causal_Inference_in_Python.pdf\", mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39bb9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = loader2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd084958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5869"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927387fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Changing gears a bit (or not at all), place yourself in the shoes of a brilliant risk ana‐ lyst. You were just hired by a lending company and your first task is to perfect its credit risk model. The goal is to have a good automated decision-making system that assesses the customers’ credit worthiness (underwrites them) and decides how much credit the company can lend them. Needless to say, errors in this system are incredi‐ bly expensive, especially if the given credit line is high.', metadata={'source': 'data/Causal_Inference_in_Python.pdf', 'coordinates': {'points': ((71.99558, 285.71403999999995), (71.99558, 359.21406999999994), (432.0044600000003, 359.21406999999994), (432.0044600000003, 285.71403999999995)), 'system': 'PixelSpace', 'layout_width': 504.0, 'layout_height': 661.5}, 'filename': 'Causal_Inference_in_Python.pdf', 'file_directory': 'data', 'last_modified': '2023-08-20T07:53:34', 'filetype': 'application/pdf', 'page_number': 14, 'category': 'NarrativeText'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c80c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b529cbc0",
   "metadata": {},
   "source": [
    "##  2A. Document Splitting Options\n",
    "\n",
    "Two options for PDF are:\n",
    "\n",
    "\n",
    "### 2Aa. Splitting by Character\n",
    "\n",
    "Splitting by character involves breaking down the text into individual characters. This method is often used when you need to analyze the text at the most granular level.\n",
    "\n",
    "**When it makes sense:**\n",
    "1. **Character-Level Analysis**: If you need to perform character-level analysis, such as identifying specific symbols or characters within the text.\n",
    "2. **Language-Independent Processing**: Character-level splitting is language-agnostic, making it suitable for multilingual documents.\n",
    "3. **Code Analysis**: In the context of code (e.g., Python), character-level splitting can be useful for syntax highlighting or identifying specific operators and symbols.\n",
    "\n",
    "### 2Ab. Splitting by BPE Tokens\n",
    "\n",
    "BPE (Byte-Pair Encoding) tokenization is a method that splits the text into subword units, often balancing between word and character levels. BPE tokenization is commonly used in modern NLP models.\n",
    "\n",
    "**When it makes sense:**\n",
    "1. **Subword-Level Analysis**: BPE tokenization is useful for languages where words can be broken down into meaningful subword units. It helps in capturing the morphological structure of the words.\n",
    "2. **Compatibility with Language Models**: Many pre-trained language models use BPE tokenization. If you are planning to use such models, splitting by BPE tokens aligns with their internal tokenization.\n",
    "3. **Code Analysis**: For code analysis, BPE tokenization can capture common programming constructs and idioms, allowing for more nuanced analysis compared to character-level splitting.\n",
    "\n",
    "\n",
    "In the context of documents containing code, the choice between character splitting and BPE token splitting may depend on the specific analysis or processing you want to perform on the code. Character splitting offers a more granular view, while BPE token splitting may provide a more nuanced understanding of code constructs.\n",
    "\n",
    "### 2B Other options\n",
    "\n",
    "It seems that the second link provided did not contain relevant information about the Markdown header metadata splitter. However, I was able to gather information about the code splitter from the first link. Let's analyze the different text splitters and compare them in the context of documents that may contain code (Python) extracted from PDFs or similar sources.\n",
    "\n",
    "### 2Ba. Split by Character\n",
    "- **Usage**: Splits the text into individual characters.\n",
    "- **When to Use**: Useful when analyzing character-level patterns or when performing character-level tokenization for specific NLP tasks.\n",
    "\n",
    "### 2Bb. Split by BPE (Byte-Pair Encoding) Token\n",
    "- **Usage**: Splits the text into subword units using BPE algorithm.\n",
    "- **When to Use**: Suitable for handling out-of-vocabulary words and preserving word meaning in various languages. It can be particularly useful when dealing with code, as it can tokenize variable names and other identifiers that may not be in a standard vocabulary.\n",
    "\n",
    "### 2Bc. Code Splitter\n",
    "- **Usage**: Splits code with multiple language support, including Python, JavaScript, Markdown, Latex, etc.\n",
    "- **When to Use**: Ideal for documents containing code snippets in various programming languages. It recognizes language-specific separators and can split code accordingly.\n",
    "- **Example**: In Python, it can split by class definitions, function definitions, etc., allowing for more meaningful segmentation of code.\n",
    "\n",
    "### 2Bd. Markdown Header Metadata \n",
    "- **Usage**: Presumably, it would split text based on Markdown headers or metadata.\n",
    "- **When to Use**: Likely useful for documents written in Markdown format, where headers and metadata provide a logical structure for segmentation.\n",
    "\n",
    "### Comparison and Context Consideration\n",
    "- **BPE vs. Character Split**: BPE provides more meaningful segmentation compared to character-level splitting, especially in the context of code where variable names and syntax are important.\n",
    "- **BPE vs. Code Split**: Code Splitter is designed specifically for code and supports multiple languages, making it more suitable for documents containing code snippets. BPE might still be useful for general text within the document.\n",
    "- **BPE vs. Markdown Splitter**: Without detailed information about the Markdown splitter, it's challenging to make a direct comparison. However, if the document follows Markdown syntax, a Markdown-specific splitter might provide more structured segmentation.\n",
    "\n",
    "In summary, the choice of splitter depends on the nature of the document and the specific requirements of the task. For documents containing code, the Code Splitter seems to be the most tailored option, while BPE can be a versatile choice for mixed content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c9a1b",
   "metadata": {},
   "source": [
    "### 2Aa - Split by charachter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c557825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fbfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75773368",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([page.page_content for page in pages])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2Ab - Split by tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfe8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ''.join([page.page_content for page in pages])\n",
    "utf8_content = content.encode('utf-8', 'replace')\n",
    "texts2 = text_splitter.split_text(utf8_content.decode('utf-8'))\n",
    "len(texts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50430683",
   "metadata": {},
   "source": [
    "### 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039b0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_model.embed_documents(\n",
    "    [pages[100].page_content]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e0fa7",
   "metadata": {},
   "source": [
    "### 4. Embeddings and save to Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79398003",
   "metadata": {},
   "source": [
    "##### 4A. PDF Document Loader Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b819c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(pages, embeddings_model,persist_directory='data/chroma/1Causal_Inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a13e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful to force a save\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64487920",
   "metadata": {},
   "source": [
    "##### 4B. PDF Document Loader Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7e7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.utils import filter_complex_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95379833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db2 = Chroma.from_documents(filter_complex_metadata(data2), embeddings_model,persist_directory='data/chroma/2Causal_Inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "927d8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful to force a save\n",
    "db2.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4377790",
   "metadata": {},
   "source": [
    "##### 4C. Load each and check similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f29cb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = Chroma(persist_directory='data/chroma/1Causal_Inference',embedding_function=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "979408a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_str = \"simplify dif-in-diff covariates with OLS?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62df1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db1.similarity_search(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "373e0326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Coefficient  Formula\n",
      "The fact that you only need to residualize the treatment suggests a simpler way of\n",
      "rewriting the regression coefficient formula. In the single variable case, instead of\n",
      "using the covariance of Y and T over the variance of T, you can useβ1=ETi−Tyi\n",
      "ETi−T2.\n",
      "In the multivariate case, this would beβ1=ETi−ETXyi\n",
      "EVar TX.\n",
      "There is a difference, though. Look at the p-value. It is a bit higher than what you got\n",
      "earlier. That’s because you are not applying the denoising step, which is responsible\n",
      "for reducing variance. Still, with only the debiasing step, you can already get the\n",
      "unbiased estimate of the causal impact of credit limit on risk, given that all the con‐\n",
      "founders were included in the debiasing model.\n",
      "Y ou can also visualize what is going on by plotting the debiased version of credit limit\n",
      "against default rate. Y ou’ll see that the relationship is no longer downward sloping, as\n",
      "when the data was biased:\n",
      "Denoising Step\n",
      "While the debiasing step is crucial to estimate the correct causal effect, the denoising\n",
      "step is also nice to have, although not as important. It won’t change the value of your\n",
      "treatment effect estimate, but it will reduce its variance. In this step, you’ll regress theFrisch-Waugh-Lovell Theorem and Orthogonalization | 109\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58a22c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Chroma(persist_directory='data/chroma/2Causal_Inference',embedding_function=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5ae4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db2.similarity_search(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5aa8efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fact, to prove my point, let’s use OLS to build a synthetic control right now. All you have to do is to use y_pre_co as if it was the covariate matrix X and the column aver‐ age of y_pre_tr as the outcome y. Once you fit this model, the weights can be extrac‐ ted with .coef_:\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dee30315",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db1.similarity_search_by_vector(OpenAIEmbeddings().embed_query(new_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55dd59d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Coefficient  Formula\n",
      "The fact that you only need to residualize the treatment suggests a simpler way of\n",
      "rewriting the regression coefficient formula. In the single variable case, instead of\n",
      "using the covariance of Y and T over the variance of T, you can useβ1=ETi−Tyi\n",
      "ETi−T2.\n",
      "In the multivariate case, this would beβ1=ETi−ETXyi\n",
      "EVar TX.\n",
      "There is a difference, though. Look at the p-value. It is a bit higher than what you got\n",
      "earlier. That’s because you are not applying the denoising step, which is responsible\n",
      "for reducing variance. Still, with only the debiasing step, you can already get the\n",
      "unbiased estimate of the causal impact of credit limit on risk, given that all the con‐\n",
      "founders were included in the debiasing model.\n",
      "Y ou can also visualize what is going on by plotting the debiased version of credit limit\n",
      "against default rate. Y ou’ll see that the relationship is no longer downward sloping, as\n",
      "when the data was biased:\n",
      "Denoising Step\n",
      "While the debiasing step is crucial to estimate the correct causal effect, the denoising\n",
      "step is also nice to have, although not as important. It won’t change the value of your\n",
      "treatment effect estimate, but it will reduce its variance. In this step, you’ll regress theFrisch-Waugh-Lovell Theorem and Orthogonalization | 109\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd5c466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db2.similarity_search_by_vector(OpenAIEmbeddings().embed_query(new_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7aa076ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fact, to prove my point, let’s use OLS to build a synthetic control right now. All you have to do is to use y_pre_co as if it was the covariate matrix X and the column aver‐ age of y_pre_tr as the outcome y. Once you fit this model, the weights can be extrac‐ ted with .coef_:\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69634979",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db1.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6cb305e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_kwargs = {\"score_threshold\":0.8,\"k\":4}\n",
    "docs = retriever.get_relevant_documents(new_str,\n",
    "                                       search_kwargs=search_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfb86603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER 4\\nThe Unreasonable Effectiveness\\nof Linear Regression\\nIn this chapter you’ll add  the first major debiasing technique in your causal inference\\narsenal: linear regression or ordinary least squares (OLS) and orthogonalization.\\nY ou’ll see how linear regression can adjust for confounders when estimating the rela‐\\ntionship between a treatment and an outcome. But, more than that, I hope to equip\\nyou with the powerful concept of treatment orthogonalization. This idea, born in lin‐\\near regression, will come in handy later on when you start to use machine learning\\nmodels for causal inference.\\nAll You Need Is Linear Regression\\nBefore you skip to the next chapter because “oh, regression is so easy! It’s the first\\nmodel I learned as a data scientist” and yada yada, let me assure you that no, you\\nactually don’t know linear regression. In fact, regression is one of the most fascinat‐\\ning, powerful, and dangerous models in causal inference. Sure, it’s more than one\\nhundred years old. But, to this day, it frequently catches even the best causal inference\\nresearchers off guard.\\nOLS Research\\nDon’t believe me? Just take a look at some recently published\\npapers on the topic and you’ll see. A good place to start is the arti‐\\ncle “Difference-in-Differences with Variation in Treatment Tim‐\\ning, ” by Andrew Goodman-Bacon, or the paper “Interpreting OLS\\nEstimands When Treatment Effects Are Heterogeneous” by Tymon\\nSłoczyński, or even the paper “Contamination Bias in Linear\\nRegressions” by Goldsmith-Pinkham et al.95'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c9e04",
   "metadata": {},
   "source": [
    "### 5. Retrievers\n",
    "\n",
    "\n",
    "These retrievers offer diverse approaches to document retrieval, catering to different needs and scenarios. Whether it's handling multiple queries, compressing embeddings, combining different algorithms, or managing the granularity of documents, these retrievers provide robust solutions for various retrieval challenges.\n",
    "\n",
    "\n",
    "The **MultiQueryRetriever** is a part of Langchain's data connection retrievers. It's designed to enhance the process of distance-based vector database retrieval, which embeds queries in high-dimensional space and finds similar embedded documents based on \"distance.\" Here's a summary of its functionality and benefits:\n",
    "\n",
    "1. **Overcoming Limitations**: Traditional distance-based retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Manual prompt engineering or tuning is often done to address these problems but can be tedious.\n",
    "\n",
    "2. **Automating Prompt Tuning**: The MultiQueryRetriever automates the process of prompt tuning by using an LLM (Language Model) to generate multiple queries from different perspectives for a given user input query.\n",
    "\n",
    "3. **Richer Results**: By generating multiple perspectives on the same question, the MultiQueryRetriever might be able to overcome some of the limitations of distance-based retrieval and get a richer set of results. It retrieves a set of relevant documents for each query and takes the unique union across all queries to get a larger set of potentially relevant documents.\n",
    "\n",
    "4. **Integration with Other Components**: It can be used with other Langchain components like vector stores (e.g., Chroma), document loaders (e.g., WebBaseLoader), embeddings (e.g., OpenAIEmbeddings), and text splitters (e.g., RecursiveCharacterTextSplitter).\n",
    "\n",
    "5. **Customizable Query Generation**: Users can specify the LLM to use for query generation, and the retriever will handle the rest. Additionally, users can supply their own prompt along with an output parser to split the results into a list of queries, allowing for more customized query generation.\n",
    "\n",
    "6. **Use Cases**: Ideal for scenarios where a more comprehensive and nuanced retrieval of documents is required, and where traditional distance-based retrieval might fall short in capturing the semantics of the query.\n",
    "\n",
    "In essence, the MultiQueryRetriever offers a more sophisticated and automated approach to document retrieval, leveraging multiple queries and perspectives to provide a broader and more relevant set of documents. It integrates well with other Langchain components and offers customization and logging features to suit various use cases.\n",
    "\n",
    "\n",
    "The second document retriever from the provided links is the \"Contextual Compression Retriever.\" Here's a summary of its functionality and usage:\n",
    "\n",
    "### Contextual Compression Retriever\n",
    "\n",
    "#### Purpose:\n",
    "The Contextual Compression Retriever is designed to address the challenge of retrieving specific queries from a document storage system where the relevant information may be buried in a document with a lot of irrelevant text. It aims to reduce the cost of calls and improve responses by compressing the retrieved documents to only include relevant information.\n",
    "\n",
    "#### How It Works:\n",
    "- **Compression**: The term \"compressing\" here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "- **Components**: To use the Contextual Compression Retriever, you'll need a base Retriever and a Document Compressor.\n",
    "- **Process**: The Contextual Compression Retriever passes queries to the base Retriever, takes the initial documents, and passes them through the Document Compressor. The Document Compressor shortens the list of documents by reducing their contents or dropping them altogether.\n",
    "\n",
    "#### Example Usage:\n",
    "The example provided in the documentation demonstrates how to initialize a simple vector store retriever and store the 2023 State of the Union speech in chunks. It shows how the retriever returns relevant and irrelevant documents, and even the relevant documents may contain irrelevant information.\n",
    "\n",
    "\n",
    "#### Visualization:\n",
    "The documentation also includes a visual representation of how the Contextual Compression Retriever works, showing the process of passing queries to the base Retriever and compressing the documents.\n",
    "\n",
    "#### Conclusion:\n",
    "The Contextual Compression Retriever is a valuable tool for extracting relevant information from large documents. By compressing and filtering the content, it ensures that only the pertinent details are returned, optimizing the retrieval process.\n",
    "\n",
    "I will now proceed to retrieve information about the third document retriever from the provided links.\n",
    "\n",
    "The third document retriever from the provided links is the \"Ensemble Retriever.\" Here's a summary of its functionality and usage:\n",
    "\n",
    "### Ensemble Retriever\n",
    "\n",
    "#### Purpose:\n",
    "The Ensemble Retriever combines the results of multiple retrievers and reranks them using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm. By leveraging the strengths of different algorithms, it aims to achieve better performance than any single algorithm.\n",
    "\n",
    "#### How It Works:\n",
    "- **Combining Algorithms**: The Ensemble Retriever takes a list of retrievers as input and ensembles their results.\n",
    "- **Hybrid Search**: The most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like Embedding similarity). This approach is known as \"hybrid search.\"\n",
    "- **Strengths**: The sparse retriever excels at finding relevant documents based on keywords, while the dense retriever is adept at finding relevant documents based on semantic similarity.\n",
    "\n",
    "#### Example Usage:\n",
    "The example provided in the documentation demonstrates how to initialize the BM25 retriever and FAISS retriever, and then combine them using the Ensemble Retriever.\n",
    "\n",
    "\n",
    "#### Conclusion:\n",
    "The Ensemble Retriever offers a powerful way to combine different retrieval algorithms to enhance the search performance. By integrating both sparse and dense retrievers, it ensures a more comprehensive and accurate retrieval of relevant documents.\n",
    "\n",
    "I will now proceed to retrieve information about the fourth document retriever from the provided links.\n",
    "\n",
    "The fourth document retriever from the provided links is the \"Parent Document Retriever.\" Here's a summary of its functionality and usage:\n",
    "\n",
    "### Parent Document Retriever\n",
    "\n",
    "#### Purpose:\n",
    "The Parent Document Retriever is designed to balance the conflicting desires of having small documents for accurate embeddings and long enough documents to retain context. It achieves this by splitting and storing small chunks of data and then retrieving the parent documents (larger chunks or whole raw documents) from which the small chunks originated.\n",
    "\n",
    "#### How It Works:\n",
    "- **Splitting Documents**: The retriever splits documents into small chunks and indexes them.\n",
    "- **Retrieving Parent Documents**: During retrieval, it fetches the small chunks and then looks up the parent IDs for those chunks, returning the larger documents.\n",
    "\n",
    "#### Two Modes of Operation:\n",
    "1. **Retrieving Full Documents**: In this mode, the retriever retrieves the full documents by specifying only a child splitter.\n",
    "2. **Retrieving Larger Chunks**: If the full documents are too large, the retriever can split the raw documents into larger chunks and then further split them into smaller chunks. It indexes the smaller chunks but retrieves the larger chunks upon request.\n",
    "\n",
    "\n",
    "#### Conclusion:\n",
    "The Parent Document Retriever provides a flexible way to handle document retrieval by allowing control over the granularity of the retrieved documents. It can retrieve full documents or larger chunks, depending on the requirements, ensuring that the context is retained without losing the accuracy of embeddings.\n",
    "\n",
    "### Summary of All Four Retrievers:\n",
    "1. **MultiQueryRetriever**: Rephrases your query using LLM and then searches your vector store\n",
    "2. **Contextual Compression Retriever**: Many internal relevant documents are compressed and passed to the LLM to answer the question.\n",
    "3. **Ensemble Retriever**: Combines multiple retrievers to enhance search performance.\n",
    "4. **Parent Document Retriever**: Balances the need for small documents for accurate embeddings with the need to retain context.The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7608355",
   "metadata": {},
   "source": [
    "##### 5A. Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "654ce1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eeb60bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c6367f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db1.as_retriever(),llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7cdd0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a1bb546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiQueryRetriever(tags=None, metadata=None, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x000001A4610F0220>, search_type='similarity', search_kwargs={}), llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=LineListOutputParser(pydantic_object=<class 'langchain.retrievers.multi_query.LineList'>), return_final_only=True, llm_kwargs={}), verbose=True, parser_key='lines')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_from_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df9c21",
   "metadata": {},
   "source": [
    "##### 5B. Context Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f492624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e94b750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cfee1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever1 = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=db2.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "68a51acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWQ = 'who wrote the paper \"Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects\" and when?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8fce4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever1.get_relevant_documents(NEWQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d6ff596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects,\" by Sun and Abraham'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ae28e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.dict of Document(page_content='\"Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects,\" by Sun and Abraham', metadata={'category': 'NarrativeText', 'file_directory': 'data', 'filename': 'Causal_Inference_in_Python.pdf', 'filetype': 'application/pdf', 'last_modified': '2023-08-20T07:53:34', 'page_number': 292, 'source': 'data/Causal_Inference_in_Python.pdf'})>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs[0].dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac0257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
