{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f52c5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01519f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f23304aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../../gpt-engineer', glob=\"**/*.py\",recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca62cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efea5604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f4a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"from ..db import DB\\n\\ndef test_db(): # use /tmp for testing db = DB('/tmp/test_db') db['test'] = 'test' assert db['test'] == 'test' db['test'] = 'test2' assert db['test'] == 'test2' db['test2'] = 'test2' assert db['test2'] == 'test2' assert db['test'] == 'test2' print('test_db passed')\", metadata={'source': '..\\\\..\\\\gpt-engineer\\\\tests\\\\test_db.py'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dc1f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1d157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=500, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "715d22ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import openai', metadata={}),\n",
       " Document(page_content='class AI:\\n\\ndef __init__(self, *\\n\\n\\n\\nkwargs):\\n\\nself.kwargs = kwargs\\n\\ntry: openai.Model.retrieve(\"gpt-4\") except openai.error.InvalidRequestError: print(\"Model gpt-4 not available for provided api key reverting \" \"to gpt-3.5.turbo. Sign up for the gpt-4 wait list here: \" \"https://openai.com/waitlist/gpt-4-api\") self.kwargs[\\'model\\'] = \"gpt-3.5-turbo\"', metadata={}),\n",
       " Document(page_content='def start(self, system, user):\\n\\nmessages = [\\n\\n{\"role\": \"system\", \"content\": system},\\n\\n{\"role\": \"user\", \"content\": user},\\n\\n]\\n\\nreturn self.next(messages)\\n\\ndef fsystem(self, msg): return {\"role\": \"system\", \"content\": msg}\\n\\ndef fuser(self, msg): return {\"role\": \"user\", \"content\": msg}', metadata={}),\n",
       " Document(page_content='def next(self, messages: list[dict[str, str]], prompt=None): if prompt: messages = messages + [{\"role\": \"user\", \"content\": prompt}]\\n\\nresponse = openai.ChatCompletion.create(\\n\\nmessages=messages, stream=True, *\\n\\n\\n\\nself.kwargs\\n\\n)\\n\\nchat = [] for chunk in response: delta = chunk[\\'choices\\'][0][\\'delta\\'] msg = delta.get(\\'content\\', \\'\\') print(msg, end=\"\") chat.append(msg) return messages + [{\"role\": \"assistant\", \"content\": \"\".join(chat)}]', metadata={}),\n",
       " Document(page_content='import re\\n\\ndef parse_chat(chat):  # -> List[Tuple[str, str]]: # Get all ``` blocks regex = r\"```(.*? )```\"\\n\\nmatches = re.finditer(regex, chat, re.DOTALL)\\n\\nfiles = [] for match in matches: path = match.group(1).split(\"\\\\n\")[0] # Get the code code = match.group(1).split(\"\\\\n\")[1:] code = \"\\\\n\".join(code) # Add the file to the list files.append((path, code))\\n\\nreturn files', metadata={}),\n",
       " Document(page_content=\"def to_files(chat, workspace):\\n\\nworkspace['all_output.txt'] = chat\\n\\nfiles = parse_chat(chat) for file_name, file_content in files: workspace[file_name] = file_content\", metadata={}),\n",
       " Document(page_content=\"import os\\n\\nfrom dataclasses import dataclass\\n\\nfrom pathlib import Path\\n\\nclass DB:\\n\\ndef __init__(self, path):\\n\\nself.path = Path(path).absolute()\\n\\nos.makedirs(self.path, exist_ok=True)\\n\\ndef __getitem__(self, key): with open(self.path / key, encoding='utf-8') as f: return f.read()\\n\\ndef __setitem__(self, key, val): with open(self.path / key, 'w', encoding='utf-8') as f: f.write(val)\\n\\n# dataclass for all dbs: @dataclass class DBs: memory: DB logs: DB identity: DB input: DB workspace: DB\", metadata={}),\n",
       " Document(page_content='import json\\n\\nimport pathlib\\n\\nimport typer\\n\\nfrom ai import AI from db import DB, DBs from steps import STEPS\\n\\napp = typer.Typer()\\n\\n@app.command() def chat( project_path: str = typer.Argument(None, help=\"path\"), run_prefix: str = typer.Option( \"\", help=\"run prefix, if you want to run multiple variants of the same project and later compare them\", ), model: str = \"gpt-4\", temperature: float = 0.1, ): if project_path is None: project_path = str(pathlib.Path(__file__).parent / \"example\")', metadata={}),\n",
       " Document(page_content='input_path = project_path memory_path = pathlib.Path(project_path) / (run_prefix + \"memory\") workspace_path = pathlib.Path(project_path) / (run_prefix + \"workspace\")\\n\\nai = AI(\\n\\nmodel=model,\\n\\ntemperature=temperature,\\n\\n)\\n\\ndbs = DBs(\\n\\nmemory=DB(memory_path),\\n\\nlogs=DB(pathlib.Path(memory_path) / \"logs\"),\\n\\ninput=DB(input_path),\\n\\nworkspace=DB(workspace_path),\\n\\nidentity=DB(pathlib.Path(__file__).parent / \"identity\"),\\n\\n)\\n\\nfor step in STEPS:\\n\\nmessages = step(ai, dbs)', metadata={}),\n",
       " Document(page_content='dbs.logs[step.__name__] = json.dumps(messages)\\n\\nif __name__ == \"__main__\":\\n\\napp()', metadata={}),\n",
       " Document(page_content=\"import json\\n\\nfrom ai import AI\\n\\nfrom chat_to_files import to_files\\n\\nfrom db import DBs\\n\\ndef setup_sys_prompt(dbs): return dbs.identity['setup'] + '\\\\nUseful to know:\\\\n' + dbs.identity['philosophy']\\n\\ndef run(ai: AI, dbs: DBs): '''Run the AI on the main prompt and save the results''' messages = ai.start( setup_sys_prompt(dbs), dbs.input['main_prompt'], ) to_files(messages[-1]['content'], dbs.workspace) return messages\", metadata={}),\n",
       " Document(page_content='def clarify(ai: AI, dbs: DBs): \\'\\'\\' Ask the user if they want to clarify anything and save the results to the workspace \\'\\'\\' messages = [ai.fsystem(dbs.identity[\\'qa\\'])] user = dbs.input[\\'main_prompt\\'] while True: messages = ai.next(messages, user)\\n\\nif messages[\\n\\n\\n\\n1][\\'content\\'].strip().lower() == \\'no\\':\\n\\nbreak\\n\\nprint() user = input(\\'(answer in text, or \"q\" to move on)\\\\n\\') print()\\n\\nif not user or user == \\'q\\': break', metadata={}),\n",
       " Document(page_content='user += ( \\'\\\\n\\\\n\\' \\'Is anything else unclear? If yes, only answer in the form:\\\\n\\' \\'{remaining unclear areas} remaining questions.\\\\n\\' \\'{Next question}\\\\n\\' \\'If everything is sufficiently clear, only answer \"no\".\\' )\\n\\nprint()\\n\\nreturn messages', metadata={}),\n",
       " Document(page_content=\"def run_clarified(ai: AI, dbs: DBs): # get the messages from previous step messages = json.loads(dbs.logs[clarify.__name__])\\n\\nmessages = [\\n\\nai.fsystem(setup_sys_prompt(dbs)),\\n\\n] + messages[1:]\\n\\nmessages = ai.next(messages, dbs.identity['use_qa'])\\n\\nto_files(messages[\\n\\n\\n\\n1]['content'], dbs.workspace)\\n\\nreturn messages\\n\\nSTEPS = [clarify, run_clarified]\\n\\n# Future steps that can be added: # improve_files, # add_tests # run_tests_and_fix_files, # improve_based_on_in_file_feedback_comments\", metadata={}),\n",
       " Document(page_content='import json\\n\\nimport pathlib\\n\\nimport typer\\n\\nfrom ..ai import AI\\n\\nfrom ..chat_to_files import to_files\\n\\napp = typer.Typer()\\n\\n@app.command() def chat( messages_path: str, out_path: str | None = None, model: str = \"gpt-4\", temperature: float = 0.1, max_tokens: int = 4096, ): ai = AI( model=model, temperature=temperature, max_tokens=max_tokens, )\\n\\nwith open(messages_path) as f:\\n\\nmessages = json.load(f)\\n\\nmessages = ai.next(messages)', metadata={}),\n",
       " Document(page_content='if out_path: to_files(messages[-1][\"content\"], out_path) with open(pathlib.Path(out_path) / \"all_output.txt\", \"w\") as f: json.dump(messages[-1][\"content\"], f)\\n\\nif __name__ == \"__main__\":\\n\\napp()', metadata={}),\n",
       " Document(page_content=\"from ..db import DB\\n\\ndef test_db(): # use /tmp for testing db = DB('/tmp/test_db') db['test'] = 'test' assert db['test'] == 'test' db['test'] = 'test2' assert db['test'] == 'test2' db['test2'] = 'test2' assert db['test2'] == 'test2' assert db['test'] == 'test2' print('test_db passed')\", metadata={})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_docs = python_splitter.create_documents([d.page_content for d in docs])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a9a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
