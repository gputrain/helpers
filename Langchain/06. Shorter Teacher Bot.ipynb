{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9a8e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aaf306",
   "metadata": {},
   "source": [
    "##### 1. Load one vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6bb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get embeddings and Chroma to load the vector store\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39e6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3091458",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory='data/chroma/1Causal_Inference',embedding_function=embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d8f61",
   "metadata": {},
   "source": [
    "##### 2. Setup the OpenAI Chat LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c2400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab18d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e0d4b",
   "metadata": {},
   "source": [
    "##### 3. MultiQuery Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd65dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mq = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ef614",
   "metadata": {},
   "source": [
    "#### 4. Setup Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddba4c",
   "metadata": {},
   "source": [
    "##### 5A. My Langchan Chains toward the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_model = LLMMathChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_model.run(question='What is 3 raised to 3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbd32b",
   "metadata": {},
   "source": [
    "##### 5B. Teaching Agent using MQ for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eebcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b77c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_system_template = \"\"\"\n",
    "You are a specialized teacher chatbot with expertise in causal inference using Python. \n",
    "Your primary role is to guide students through interactive lessons, hands-on exercises, and provide thoughtful insights. \n",
    "You will help students understand the underlying principles, methodologies, and practical applications of causal inference.\n",
    "\n",
    "Your responses should be clear, concise, and tailored to the students' level of understanding, encouraging \n",
    "them to think critically and engage with the material. \n",
    "Encourage questions and provide examples where necessary to illustrate complex ideas.\n",
    "\n",
    "When explaining concepts, refer to the internal documents by citing page numbers or section headers as provided from the \n",
    "vector store. Do not reference any external links or sources outside the provided internal documents. \n",
    "Ensure you added page numbers at the end of your response without exception? Citing content is a must.\n",
    "\"\"\"\n",
    "\n",
    "teacher_human_template = 'As a teacher of this book answer this {question}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d85f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([teacher_system_template,teacher_human_template])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_llm = LLMChain(llm=llm, prompt=chat_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the benefits of OLS regression with regards to casuality analysis?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_llm.run(input_documents=mq.get_relevant_documents(question),question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855dc7a1",
   "metadata": {},
   "source": [
    "##### 5C. Reasoning Agent using MQ for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_system_template = \"\"\"\n",
    "You are an autoregressive language model that has been fine-tuned with instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning. If you think there might not be a correct answer, you say so.\n",
    "\n",
    "Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question.\n",
    "\n",
    "Your users are experts in AI and ethics, so they already know you're a language model and your capabilities and limitations, so don't remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either.\n",
    "\n",
    "Don't be verbose in your answers, but do provide details and examples where it might help the explanation. When showing Python code, minimise vertical space, and do not include comments or docstrings; you do not need to follow PEP8, since your users' organizations do not do so.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_human_template = 'Using all of your reasoning skills answer this {question}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template2 = ChatPromptTemplate.from_messages([reasoning_system_template,reasoning_human_template])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasnoning_llm = LLMChain(llm=llm, prompt=chat_prompt_template2,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ae500",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is OLS relevant in casuality analysis?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e24dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasnoning_llm.run({'input_documents':mq.get_relevant_documents(question),\n",
    "                    'question':question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e87e0",
   "metadata": {},
   "source": [
    "##### 5D. Current Time Tool as a Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# DOC STRINGS SHOULD BE VERY DESCRIPTIVE\n",
    "# IT IS WHAT THE LLM READS TO DECIDE TO USE THE TOOL!\n",
    "@tool\n",
    "def get_time(text: str) -> str:\n",
    "    '''Returns the current time. Use this for any questions\n",
    "    regarding the current time. Input is an empty string and\n",
    "    the current time is returned in a string format. Only use this function\n",
    "    for the current time. Other time related questions should use another tool'''\n",
    "    return str(datetime.now()).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bfb27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33439ede",
   "metadata": {},
   "source": [
    "###### 6. Setup Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e480fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022633c5",
   "metadata": {},
   "source": [
    "##### 7. Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ceb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools = load_tools([\"llm-math\"], llm=llm)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Specialised teacher chatbot\",\n",
    "        func=teacher_llm.run,#(input_documents=mq.get_relevant_documents(question),question=question)\n",
    "        description=\"useful for when you have a specific question around causal analysis, stats, machine learning, etc\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Specialised reasoning chatbot\",\n",
    "        func=reasnoning_llm.run, #(input_documents=mq.get_relevant_documents(question),question=question),\n",
    "        description=\"useful for questions that require reasoning skills\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "    \n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b356b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0)\n",
    "agent = initialize_agent(tools, \n",
    "                         chat_model, \n",
    "                         agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         input_documents=mq.get_relevant_documents(input),\n",
    "                         memory=memory)\n",
    "agent.run(input = \"What is the title of the book you are a teacher on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae081e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(input = \"explain to me what that book is about ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb018cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f8cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
